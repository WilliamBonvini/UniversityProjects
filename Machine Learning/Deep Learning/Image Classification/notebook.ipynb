{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)  \n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth\n",
    "# Allows to only as much GPU memory as needed\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "apply_data_augmentation = True\n",
    "\n",
    "# Create training ImageDataGenerator object\n",
    "if apply_data_augmentation:\n",
    "    train_data_gen = ImageDataGenerator(rotation_range=10,\n",
    "                                        width_shift_range=10,\n",
    "                                        height_shift_range=10,\n",
    "                                        zoom_range=0.3,\n",
    "                                        horizontal_flip=True,\n",
    "                                        vertical_flip=True,\n",
    "                                        fill_mode='constant',\n",
    "                                        validation_split=0.2,\n",
    "                                        rescale=1./255)\n",
    "else:\n",
    "    train_data_gen = ImageDataGenerator(validation_split=0.2,\n",
    "                                        rescale=1./255)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1247 images belonging to 20 classes.\n",
      "Found 307 images belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create generators to read images from dataset directory\n",
    "# -------------------------------------------------------\n",
    "cwd=os.getcwd()\n",
    "dataset_dir='/kaggle/input/ann-and-dl-image-classification/Classification_Dataset'\n",
    "# img\n",
    "\n",
    "# Batch size\n",
    "bs = 32\n",
    "\n",
    "# img shape\n",
    "img_h = 256\n",
    "img_w = 256\n",
    "\n",
    "\n",
    "class_list=['owl',\n",
    "            'galaxy',\n",
    "            'lightning',\n",
    "            'wine-bottle',\n",
    "            't-shirt',\n",
    "            'waterfall',\n",
    "            'sword',\n",
    "            'school-bus',\n",
    "            'calculator',\n",
    "            'sheet-music',\n",
    "            'airplanes',\n",
    "            'lightbulb',\n",
    "            'skyscraper',\n",
    "            'mountain-bike',\n",
    "            'fireworks',\n",
    "            'computer-monitor',\n",
    "            'bear',\n",
    "            'grand-piano',\n",
    "            'kangaroo',\n",
    "            'laptop']\n",
    "\n",
    "\n",
    "training_dir = os.path.join(dataset_dir, 'training')\n",
    "# Training\n",
    "train_gen = train_data_gen.flow_from_directory(training_dir,\n",
    "                                               batch_size=bs, \n",
    "                                               class_mode='categorical',\n",
    "                                               classes=class_list,\n",
    "                                               color_mode='rgb',\n",
    "                                               shuffle=True,\n",
    "                                               seed=SEED,\n",
    "                                               subset='training')  # targets are directly converted into one-hot vectors\n",
    "#Validation\n",
    "valid_gen = train_data_gen.flow_from_directory(training_dir,\n",
    "                                             batch_size=bs,\n",
    "                                             class_mode='categorical',\n",
    "                                             classes=class_list,\n",
    "                                             color_mode='rgb',\n",
    "                                             shuffle=True,\n",
    "                                             seed=SEED,\n",
    "                                             subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataset object for\n",
    "num_classes=20\n",
    "\n",
    "# Training\n",
    "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
    "                                               output_types=(tf.float32, tf.float32),                                   \n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
    "\n",
    "# repeat:\n",
    "# Without calling the repeat function the dataset \n",
    "# will be empty after consuming all the images\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "\n",
    "# Validation\n",
    "# ----------\n",
    "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
    "                                               output_types=(tf.float32, tf.float32),\n",
    "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
    "# repeat:\n",
    "valid_dataset = valid_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load VGG16 Model\n",
    "\n",
    "vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 128, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fd145d71358>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd1286c2e48>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd12857b748>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fd128546f60>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd16402acc0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd1285007f0>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fd12850aac8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd12850aba8>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd128530358>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd1284bc630>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fd1284ce8d0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd1284ce940>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd1284e1fd0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd128487438>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fd128493710>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd1284937f0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd1284a5f60>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd128451278>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7fd12845d550>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.summary()\n",
    "vgg.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 8, 8, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               16777728  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 31,502,676\n",
      "Trainable params: 23,867,412\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create Model\n",
    "# ------------\n",
    "\n",
    "finetuning = True\n",
    "\n",
    "if finetuning:\n",
    "    freeze_until = 15 # layer from which we want to fine-tune\n",
    "    \n",
    "    for layer in vgg.layers[:freeze_until]:\n",
    "        layer.trainable = False\n",
    "else:\n",
    "    vgg.trainable = False\n",
    "    \n",
    "model = tf.keras.Sequential()\n",
    "model.add(vgg)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Visualize created model as a table\n",
    "model.summary()\n",
    "\n",
    "# Visualize initialized weights\n",
    "#model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization params\n",
    "# -------------------\n",
    "\n",
    "# Loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "# -------------------\n",
    "\n",
    "# Validation metrics\n",
    "# ------------------\n",
    "\n",
    "metrics = ['accuracy']\n",
    "# ------------------\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 39 steps, validate for 10 steps\n",
      "Epoch 1/100\n",
      "39/39 [==============================] - 840s 22s/step - loss: 3.5501 - accuracy: 0.0569 - val_loss: 2.9927 - val_accuracy: 0.0651\n",
      "Epoch 2/100\n",
      "39/39 [==============================] - 830s 21s/step - loss: 2.9960 - accuracy: 0.0634 - val_loss: 2.9880 - val_accuracy: 0.0651\n",
      "Epoch 3/100\n",
      "39/39 [==============================] - 809s 21s/step - loss: 2.9881 - accuracy: 0.0642 - val_loss: 2.9842 - val_accuracy: 0.0651\n",
      "Epoch 4/100\n",
      "39/39 [==============================] - 807s 21s/step - loss: 2.9849 - accuracy: 0.0561 - val_loss: 2.9823 - val_accuracy: 0.0651\n",
      "Epoch 5/100\n",
      "39/39 [==============================] - 814s 21s/step - loss: 2.9823 - accuracy: 0.0642 - val_loss: 2.9795 - val_accuracy: 0.0651\n",
      "Epoch 6/100\n",
      "39/39 [==============================] - 807s 21s/step - loss: 2.9804 - accuracy: 0.0642 - val_loss: 2.9750 - val_accuracy: 0.0651\n",
      "Epoch 7/100\n",
      "39/39 [==============================] - 801s 21s/step - loss: 2.9795 - accuracy: 0.0642 - val_loss: 2.9749 - val_accuracy: 0.0651\n",
      "Epoch 8/100\n",
      "39/39 [==============================] - 810s 21s/step - loss: 2.9788 - accuracy: 0.0593 - val_loss: 2.9750 - val_accuracy: 0.0651\n",
      "Epoch 9/100\n",
      "39/39 [==============================] - 815s 21s/step - loss: 2.9781 - accuracy: 0.0642 - val_loss: 2.9713 - val_accuracy: 0.0651\n",
      "Epoch 10/100\n",
      "39/39 [==============================] - 805s 21s/step - loss: 2.9780 - accuracy: 0.0601 - val_loss: 2.9761 - val_accuracy: 0.0651\n",
      "Epoch 11/100\n",
      "39/39 [==============================] - 812s 21s/step - loss: 2.9781 - accuracy: 0.0569 - val_loss: 2.9741 - val_accuracy: 0.0651\n",
      "Epoch 12/100\n",
      "39/39 [==============================] - 818s 21s/step - loss: 2.9782 - accuracy: 0.0553 - val_loss: 2.9769 - val_accuracy: 0.0651\n",
      "Epoch 13/100\n",
      "39/39 [==============================] - 812s 21s/step - loss: 2.9781 - accuracy: 0.0642 - val_loss: 2.9737 - val_accuracy: 0.0651\n",
      "Epoch 14/100\n",
      "39/39 [==============================] - 818s 21s/step - loss: 2.9780 - accuracy: 0.0642 - val_loss: 2.9724 - val_accuracy: 0.0651\n",
      "Epoch 15/100\n",
      "39/39 [==============================] - 817s 21s/step - loss: 2.9778 - accuracy: 0.0577 - val_loss: 2.9738 - val_accuracy: 0.0651\n",
      "Epoch 16/100\n",
      "39/39 [==============================] - 829s 21s/step - loss: 2.9780 - accuracy: 0.0609 - val_loss: 2.9723 - val_accuracy: 0.0651\n",
      "Epoch 17/100\n",
      "39/39 [==============================] - 826s 21s/step - loss: 2.9781 - accuracy: 0.0642 - val_loss: 2.9750 - val_accuracy: 0.0651\n",
      "Epoch 18/100\n",
      "39/39 [==============================] - 825s 21s/step - loss: 2.9779 - accuracy: 0.0593 - val_loss: 2.9747 - val_accuracy: 0.0651\n",
      "Epoch 19/100\n",
      "39/39 [==============================] - 820s 21s/step - loss: 2.9776 - accuracy: 0.0577 - val_loss: 2.9741 - val_accuracy: 0.0651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd127e92438>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks=[]\n",
    "# Early Stopping\n",
    "# --------------\n",
    "early_stop = True\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    callbacks.append(es_callback)\n",
    "\n",
    "\n",
    "model.fit(x=train_dataset,\n",
    "          epochs=100,  #### set repeat in training dataset\n",
    "          steps_per_epoch=len(train_gen),\n",
    "          validation_data=valid_dataset,\n",
    "          validation_steps=len(valid_gen), \n",
    "          callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
